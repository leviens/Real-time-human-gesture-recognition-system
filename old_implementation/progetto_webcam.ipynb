{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecdd706e-ee9c-4007-aa13-46609eae7b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Core notebook of the project. Here there is the webcam implementation, and the data processing done in real time utilizing the\n",
    "#mediapipe hand landmarker and the classifier trained by me on its outputs\n",
    "\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e43879c8-5631-41e7-a6d4-e8bcc642081c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mediapipe hand landmark visualization function, inputs: rgb image, results from the hand landmarker object\n",
    "#output: modified image, with the hand landmarkers drawn on\n",
    "\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "\n",
    "MARGIN = 10  # pixels\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1\n",
    "HANDEDNESS_TEXT_COLOR = (88, 205, 54) # vibrant green\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "  hand_landmarks_list = detection_result.hand_landmarks\n",
    "  handedness_list = detection_result.handedness\n",
    "  annotated_image = np.copy(rgb_image)\n",
    "\n",
    "  # Loop through the detected hands to visualize.\n",
    "  for idx in range(len(hand_landmarks_list)):\n",
    "    hand_landmarks = hand_landmarks_list[idx]\n",
    "    handedness = handedness_list[idx]\n",
    "\n",
    "    # Draw the hand landmarks.\n",
    "    hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    hand_landmarks_proto.landmark.extend([\n",
    "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n",
    "    ])\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "      annotated_image,\n",
    "      hand_landmarks_proto,\n",
    "      solutions.hands.HAND_CONNECTIONS,\n",
    "      solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "      solutions.drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "    # Get the top left corner of the detected hand's bounding box.\n",
    "    height, width, _ = annotated_image.shape\n",
    "    x_coordinates = [landmark.x for landmark in hand_landmarks]\n",
    "    y_coordinates = [landmark.y for landmark in hand_landmarks]\n",
    "    text_x = int(min(x_coordinates) * width)\n",
    "    text_y = int(min(y_coordinates) * height) - MARGIN\n",
    "\n",
    "    # Draw handedness (left or right hand) on the image.\n",
    "    cv2.putText(annotated_image, f\"{handedness[0].category_name}\",\n",
    "                (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n",
    "                FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n",
    "\n",
    "  return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a4236db-9b82-4c4c-8090-1edac83feba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "#Downloads the mediapipe model from the internet\n",
    "import requests\n",
    "\n",
    "url = \"https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task\"\n",
    "response = requests.get(url)\n",
    "\n",
    "with open(\"hand_landmarker.task\", \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "print(\"File downloaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77ab9a61-2365-47fa-b176-61e85f23187c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded succesfully\n"
     ]
    }
   ],
   "source": [
    "#downloads the model trained in the other notebook from the github\n",
    "url = 'https://github.com/leviens/Live-human-gesture-recognition-system/raw/refs/heads/main/model/model_final.h5'\n",
    "\n",
    "# Download the file and save it locally\n",
    "response = requests.get(url)\n",
    "with open('model_final.h5', 'wb') as f:\n",
    "    f.write(response.content)\n",
    "print(\"File downloaded succesfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aac5a4b9-59a4-4410-ae4a-e5e4dfa215d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a hand landmarker object\n",
    "base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
    "options = vision.HandLandmarkerOptions(base_options=base_options,\n",
    "                                       num_hands=2)\n",
    "detector = vision.HandLandmarker.create_from_options(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f9888a4-5ee4-4b83-9c02-4015c06e809d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "#Setup the libraries and model\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "#loaded_model = tf.keras.models.load_model(os.path.join('D:\\progetto_video\\model', 'model_deeper.h5'))\n",
    "loaded_model = tf.keras.models.load_model('model_final.h5')\n",
    "\n",
    "class_names = {\n",
    "    -1: 'No sign',\n",
    "    1: 'Thumb up',\n",
    "    2: 'V sign',\n",
    "    3: 'Three',\n",
    "    4: 'Four',\n",
    "    5: 'Five',\n",
    "    6: 'Ok',\n",
    "    7: 'No sign'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e82f8803-944c-4f88-9f9d-3f911bbdf6aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Core part. Run this to use the webcam integration system\n",
    "#Works on one hand at a time\n",
    "#Press q to close the webcam window\n",
    "\n",
    "def main():\n",
    "    # Open a connection to the webcam (0 is usually the built-in webcam)\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        return\n",
    "    \n",
    "    # Set the desired resolution (e.g., 1280x720)\n",
    "    width = 1280 #1920 \n",
    "    height = 720 #1080\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "\n",
    "    # Resize the display window\n",
    "    cv2.namedWindow('Webcam Feed', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Webcam Feed', width, height)\n",
    "\n",
    "    while True:\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Could not read frame.\")\n",
    "            break\n",
    "\n",
    "        #Trasform image into mediapipe object to run the hand landmarker\n",
    "        image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n",
    "        detection_result = detector.detect(image)\n",
    "        img = draw_landmarks_on_image(frame, detection_result)\n",
    "        \n",
    "        hand_landmarks_list = detection_result.hand_landmarks\n",
    "        handedness_list = detection_result.handedness\n",
    "        mano=False\n",
    "        #Extract the data that will be passed as input to the classifier\n",
    "        for idx in range(len(hand_landmarks_list)):\n",
    "            hand_landmarks = hand_landmarks_list[idx]\n",
    "            handedness = handedness_list[idx][0].index\n",
    "            #ignores left hand\n",
    "            #if handedness == 1:\n",
    "                #continue\n",
    "            mano=True\n",
    "            x_coordinates = [landmark.x for landmark in hand_landmarks]\n",
    "            y_coordinates = [landmark.y for landmark in hand_landmarks]\n",
    "            z_coordinates = [landmark.z for landmark in hand_landmarks]\n",
    "        #Process the raw data\n",
    "        if mano:\n",
    "            features = [handedness] + x_coordinates + y_coordinates + z_coordinates\n",
    "            #features = features.astype('float32')\n",
    "            features = np.array(features)\n",
    "            features /= np.max(features)\n",
    "            features = features.reshape(1, -1)\n",
    "            #Run the neural network and obtain prediction probabilities\n",
    "            predicted_class = loaded_model.predict(features)\n",
    "\n",
    "            #Select the higher probability class\n",
    "            prob = predicted_class[0,:]\n",
    "            classe = np.argmax(prob)\n",
    "            if prob[classe] <0.85:\n",
    "                classe = -2\n",
    "            classe+=1\n",
    "    \n",
    "            # Display the resulting frame with the classification result\n",
    "            class_name = class_names.get(classe)\n",
    "            \n",
    "            cv2.putText(img, f'Sign: {class_name}', (10, 30), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('Webcam Feed', img)\n",
    "\n",
    "        # Break the loop on 'q' key press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    # When everything done, release the capture\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ace2d1-f663-435f-985d-6b10990cbfad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
