{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9551124b-1eb8-4601-8926-09cdca05111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core notebook of the project.\n",
    "# The following blocks contains the live webcam implementation of the classifier built upon the mediapipe hand landmarker model.\n",
    "# First, the trained classifier is loaded, and then the mediapipe hands model.\n",
    "# The main function then accesses the local webcam, processes the images frame by frame,\n",
    "# and outputs the hand gesture being executed by the user, if any. \n",
    "\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "477a9705-3575-4d2a-ba5b-5ff4c87f0a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded succesfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "#downloads the model trained in the other notebook from the github\n",
    "url = 'https://github.com/leviens/Live-human-gesture-recognition-system/raw/refs/heads/main/model/model_new.h5'\n",
    "\n",
    "# Download the file and save it locally\n",
    "response = requests.get(url)\n",
    "with open('model_final.h5', 'wb') as f:\n",
    "    f.write(response.content)\n",
    "print(\"File downloaded succesfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "27995a90-e640-4485-aa35-d7400a0de47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "#Setup the libraries and load the model\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "#loaded_model = tf.keras.models.load_model('D:\\progetto_video\\model_new\\model_new.h5')\n",
    "\n",
    "loaded_model = tf.keras.models.load_model('model_final.h5')\n",
    "\n",
    "# Define the name of the gestures\n",
    "# Additional info on the gestures on the github documentation\n",
    "\n",
    "class_names = {\n",
    "    -1: 'No sign',\n",
    "    1: 'Thumb up',\n",
    "    2: 'V sign',\n",
    "    3: 'Three',\n",
    "    4: 'Four',\n",
    "    5: 'Five',\n",
    "    6: 'Ok',\n",
    "    7: 'Rock sign',\n",
    "    8: 'No sign'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6df10d2b-0ab2-414f-90d7-c456d481da99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the mediapipe hands model and the drawing utils\n",
    "\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "def draw_landmarks_on_image(image, hand_landmarks):\n",
    "    \"\"\"Draws hand landmarks on the provided image.\"\"\"\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "17ab4019-a0d5-4234-8275-714630813c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n"
     ]
    }
   ],
   "source": [
    "# Main function: run this block to use the real time webcam gesture recognition system.\n",
    "# NOTE: close the window by pressing the \"q\" key on your keyboard.\n",
    "# Classification results are written on the webcam window: in the upper left corner for the left hand, in the upper right corner for the right hand.\n",
    "# The implementation works simultaneusly on both hands.\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Captures video from webcam, performs hand landmark detection,\n",
    "    and classifies hand gestures in live stream mode.\"\"\"\n",
    "\n",
    "    # Open a connection to the webcam:\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        return\n",
    "\n",
    "    # Set the desired resolution. Works with multiple resolutions, may need to adapt the text writing function at the end. It should work \n",
    "    # with HD and full HD with those settings\n",
    "    width =  1280 #1920 \n",
    "    height = 720 #1080 \n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "\n",
    "    # Resize the display window\n",
    "    cv2.namedWindow('Webcam Feed', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Webcam Feed', width, height)\n",
    "\n",
    "    # main loop\n",
    "    with mp_hands.Hands(\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5) as hands: # mediapipe model\n",
    "        while True:\n",
    "            # Capture frame-by-frame\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Error: Could not read frame.\")\n",
    "                break\n",
    "\n",
    "            # Flip the image horizontally for a later selfie-view display\n",
    "            frame = cv2.flip(frame, 1)\n",
    "\n",
    "            # Convert frame to RGB format for MediaPipe\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False # Improve performance\n",
    "            results = hands.process(image) # Extract hand landmarks\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # Works on both hands, simultaneusly\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks, handedness_info in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "                    handedness = handedness_info.classification[0].label\n",
    "\n",
    "                    # Extract x, y, and z coordinates of landmarks\n",
    "                    x_coordinates = [landmark.x for landmark in hand_landmarks.landmark]\n",
    "                    y_coordinates = [landmark.y for landmark in hand_landmarks.landmark]\n",
    "                    z_coordinates = [landmark.z for landmark in hand_landmarks.landmark]\n",
    "\n",
    "                    # Process the raw data: eventual normalization procedure. Not done here, see documentations\n",
    "                    \n",
    "                    # Convert handedness to numerical representation\n",
    "                    handedness_num = 0 if handedness == \"Right\" else 1 # 1 for right, 0 for left\n",
    "                    features = [handedness_num] + x_coordinates + y_coordinates + z_coordinates\n",
    "                    features = np.array(features, dtype=np.float32) # Crucial: specify dtype\n",
    "\n",
    "                    features = features.reshape(1, -1)\n",
    "\n",
    "                    # Predict gesture class using the loaded model\n",
    "                    predicted_class = loaded_model.predict(features)\n",
    "                \n",
    "                    # Select the higher probability class\n",
    "                    prob = predicted_class[0, :]\n",
    "                    classe = np.argmax(prob)\n",
    "                    if prob[classe] < 0.85:\n",
    "                        classe = -2\n",
    "                    classe += 1\n",
    "\n",
    "                    # Draw the landmarks (optional)\n",
    "                    image = draw_landmarks_on_image(image, hand_landmarks)\n",
    "\n",
    "                    # Display the classification result on the frame: upper left corner for the left hand, upper right corner for the right hand\n",
    "                    class_name = class_names.get(classe)\n",
    "                    text_x = 30 if handedness == \"Left\" else image.shape[1] - 400  # Left or Right corner\n",
    "                    text_y = 50 # y position of the text\n",
    "                    \n",
    "                    cv2.putText(image, f'Sign: {class_name}', (text_x, text_y),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 255), 3, cv2.LINE_AA)\n",
    "\n",
    "            cv2.imshow('Webcam Feed', image)\n",
    "\n",
    "            # Break the loop on 'q' key press\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    # When everything done, release the capture\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae94f19-1b11-4b92-b8aa-109968bce99a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
